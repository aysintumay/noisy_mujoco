{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from cost_func import compute_acp_cost, compute_map_physician_air, is_stable, compute_hr_physician_air, compute_pulsatility_physician_air, unstable_percentage, aggregate_air_physician, weaning_score_physician\n",
    "from reward_func import compute_reward_smooth\n",
    "from model import WorldModel\n",
    "\n",
    "DATA_PATH = \"/abiomed/downsampled/10min_1hr_window.pkl\"\n",
    "#this is just for the visualization so it isn't messy\n",
    "MAX_STEPS_TO_PLOT = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are ['train', 'val', 'test', 'mean', 'std'] keys in dataset\n",
      "episodes: 4309\n",
      "shape is torch.Size([1266, 12, 13])\n"
     ]
    }
   ],
   "source": [
    "with open(DATA_PATH, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "#this is a check to see what is included in the pkl file and hoow many episodes\n",
    "print(f\"there are {list(data.keys())} keys in dataset\")\n",
    "episodes = data['train']\n",
    "print(f\"episodes: {len(episodes)}\")\n",
    "print(f\"shape is {data['test'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to numpy if tensor\n",
    "if isinstance(episodes, torch.Tensor):    \n",
    "    episodes_np = episodes.numpy()  \n",
    "else:\n",
    "    episodes_np = np.array(episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ACP per timestep accounting for between episodes: 0.018758\n"
     ]
    }
   ],
   "source": [
    "actions_all = episodes_np[:, :, -1]\n",
    "\n",
    "\n",
    "#in order to account for the actions between episodes i am just treating the entire time series like an episode\n",
    "flattened_actions = actions_all.flatten()[:2400]\n",
    "flattened_acp = compute_acp_cost(flattened_actions)\n",
    "\n",
    "#this is averaged across all actions since the acp function called sums acp per episode\n",
    "acp_per_timestep = flattened_acp/(len(flattened_actions)-1)\n",
    "print(f\"Mean ACP per timestep accounting for between episodes: {acp_per_timestep:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[88.38285243 88.31538785 87.47272021 ... 59.83359985 51.69995995\n",
      " 48.56809362]\n",
      "Overall AIR across all timesteps accounting for between episodes for MAP: 0.022451\n"
     ]
    }
   ],
   "source": [
    "flattened_actions = episodes_np[:, :, -1].flatten()\n",
    "#i am just using all of the steps instead of the split up episodes\n",
    "flattened_map = episodes_np[:, :, 0].flatten()\n",
    "overall_map_score = compute_map_physician_air(flattened_map, flattened_actions)\n",
    "\n",
    "print(flattened_map)\n",
    "if overall_map_score is not None:\n",
    "    print(f\"Overall AIR across all timesteps accounting for between episodes for MAP: {overall_map_score:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall AIR across all timesteps accounting for between episodes for HR: 0.055172\n"
     ]
    }
   ],
   "source": [
    "flattened_actions = episodes_np[:, :, -1].flatten()\n",
    "flattened_hr = episodes_np[:, :, 9].flatten()\n",
    "overall_hr_score = compute_hr_physician_air(flattened_hr, flattened_actions)\n",
    "if overall_hr_score is not None:\n",
    "    print(f\"Overall AIR across all timesteps accounting for between episodes for HR: {overall_hr_score:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall AIR across all timesteps accounting for between episodes for pulsatility: 0.037175\n"
     ]
    }
   ],
   "source": [
    "flattened_actions = episodes_np[:, :, -1].flatten()\n",
    "flattened_pulsatility = episodes_np[:, :, 7].flatten()\n",
    "overall_pulsatility_score = compute_pulsatility_physician_air(flattened_pulsatility, flattened_actions)\n",
    "if overall_pulsatility_score is not None:\n",
    "    print(f\"Overall AIR across all timesteps accounting for between episodes for pulsatility: {overall_pulsatility_score:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of unstable states: 36.95753074959388\n"
     ]
    }
   ],
   "source": [
    "states_all = episodes_np[:, :, :-1]\n",
    "flattened_states = states_all.reshape(-1, states_all.shape[-1])\n",
    "unstable_percentage = unstable_percentage(flattened_states)\n",
    "print(f\"Percentage of unstable states: {unstable_percentage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall weaning score -0.02411634756995582\n"
     ]
    }
   ],
   "source": [
    "overall_weaning_score = weaning_score_physician(flattened_states, flattened_actions)\n",
    "print(f\"Overall weaning score {overall_weaning_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregate AIR: 0.030141287284144426\n"
     ]
    }
   ],
   "source": [
    "total_air = aggregate_air_physician(flattened_states, flattened_actions)\n",
    "print(f\"Aggregate AIR: {total_air}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total timesteps: 51708\n",
      "Created 2154 bins with 24 steps\n",
      "Mean total normalized reward per episode from all bins: -3.867591\n"
     ]
    }
   ],
   "source": [
    "all_steps_np = np.concatenate(episodes_np, axis=0)\n",
    "total_steps_available = all_steps_np.shape[0]\n",
    "print(f\"Total timesteps: {total_steps_available}\")\n",
    "\n",
    "# Group the data in bins with 24 steps\n",
    "episode_length = 24\n",
    "num_bins = total_steps_available // episode_length\n",
    "#creates bins and fits the samples into 3D\n",
    "binned_steps = all_steps_np[:num_bins * episode_length].reshape(num_bins, episode_length, -1)\n",
    "print(f\"Created {num_bins} bins with {episode_length} steps\")\n",
    "# Randomly select 100 of these bins\n",
    "num_episodes_to_sample = 633\n",
    "#no replacement\n",
    "sampled_bin_indices = np.random.choice(num_bins, num_episodes_to_sample, replace=False)\n",
    "\n",
    "all_episode_rewards = []\n",
    "\n",
    "# Loop through the 100 randomly selected bins\n",
    "for bin_idx in sampled_bin_indices:\n",
    "    # Get one bin\n",
    "    episode_bin = binned_steps[bin_idx]\n",
    "    \n",
    "    normalized_rewards_for_episode = []\n",
    "    # Loop through each of the 24 steps in the bin\n",
    "    for step_data in episode_bin:\n",
    "        step_tensor = torch.tensor(step_data, dtype=torch.float32)\n",
    "        raw_reward = compute_reward_smooth(step_tensor.unsqueeze(0))\n",
    "        \n",
    "        # Apply normalization from rl_env\n",
    "        norm_reward = (raw_reward + 4) / 5\n",
    "        clipped_reward = np.clip(norm_reward, -1.0, 1.0)\n",
    "        normalized_rewards_for_episode.append(clipped_reward)\n",
    "    \n",
    "    # Find the total reward for this simulated episode\n",
    "    total_episode_reward = sum(normalized_rewards_for_episode)\n",
    "    all_episode_rewards.append(total_episode_reward)\n",
    "\n",
    "mean_total_episode_reward = np.mean(all_episode_rewards)\n",
    "\n",
    "print(f\"Mean total normalized reward per episode from all bins: {mean_total_episode_reward:.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
