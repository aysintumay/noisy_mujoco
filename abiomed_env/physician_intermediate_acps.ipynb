{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from cost_func import compute_acp_cost, overall_map_cost, compute_map_air, unstable_states, compute_pulsatility_air, overall_pulsatility_cost, overall_hr_cost, compute_hr_air\n",
    "from reward_func import compute_reward_smooth\n",
    "from model import WorldModel\n",
    "\n",
    "DATA_PATH = \"/abiomed/downsampled/10min_1hr_window.pkl\"\n",
    "#this is just for the visualization so it isn't messy\n",
    "MAX_STEPS_TO_PLOT = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are ['train', 'val', 'test', 'mean', 'std'] keys in dataset\n",
      "episodes: 1266\n"
     ]
    }
   ],
   "source": [
    "with open(DATA_PATH, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "#this is a check to see what is included in the pkl file and hoow many episodes\n",
    "print(f\"there are {list(data.keys())} keys in dataset\")\n",
    "episodes = data['test']\n",
    "print(f\"episodes: {len(episodes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to numpy if tensor\n",
    "if isinstance(episodes, torch.Tensor):    \n",
    "    episodes_np = episodes.numpy()  \n",
    "else:\n",
    "    episodes_np = np.array(episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ACP per timestep accounting for between episodes: 0.051478\n"
     ]
    }
   ],
   "source": [
    "actions_all = episodes_np[:, :, -1]\n",
    "\n",
    "#in order to account for the actions between episodes i am just treating the entire time series like an episode\n",
    "flattened_actions = actions_all.flatten()\n",
    "flattened_acp = compute_acp_cost(flattened_actions)\n",
    "\n",
    "#this is averaged across all actions since the acp function called sums acp per episode\n",
    "acp_per_timestep = flattened_acp/(len(flattened_actions)-1)\n",
    "print(f\"Mean ACP per timestep accounting for between episodes: {acp_per_timestep:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall AIR across all timesteps accounting for between episodes for MAP: 0.013110\n"
     ]
    }
   ],
   "source": [
    "flattened_actions = episodes_np[:, :, -1].flatten()\n",
    "#i am just using all of the steps instead of the split up episodes\n",
    "flattened_map = episodes_np[:, :, 0].flatten()\n",
    "overall_air_score = compute_map_air(flattened_map, flattened_actions)\n",
    "if overall_air_score is not None:\n",
    "    print(f\"Overall AIR across all timesteps accounting for between episodes for MAP: {overall_air_score:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall AIR across all timesteps accounting for between episodes for HR: 0.015680\n"
     ]
    }
   ],
   "source": [
    "flattened_actions = episodes_np[:, :, -1].flatten()\n",
    "#i am just using all of the steps instead of the split up episodes\n",
    "flattened_map = episodes_np[:, :, 5].flatten()\n",
    "overall_air_score = compute_map_air(flattened_map, flattened_actions)\n",
    "if overall_air_score is not None:\n",
    "    print(f\"Overall AIR across all timesteps accounting for between episodes for HR: {overall_air_score:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall AIR across all timesteps accounting for between episodes for pulsatility: 0.014478\n"
     ]
    }
   ],
   "source": [
    "flattened_actions = episodes_np[:, :, -1].flatten()\n",
    "#i am just using all of the steps instead of the split up episodes\n",
    "flattened_map = episodes_np[:, :, 7].flatten()\n",
    "overall_air_score = compute_map_air(flattened_map, flattened_actions)\n",
    "if overall_air_score is not None:\n",
    "    print(f\"Overall AIR across all timesteps accounting for between episodes for pulsatility: {overall_air_score:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of unstable states: 37.13796735123749\n"
     ]
    }
   ],
   "source": [
    "states_all = episodes_np[:, :, :-1]\n",
    "flattened_states = states_all.reshape(-1, states_all.shape[-1])\n",
    "unstable_percentage = unstable_states(flattened_states)\n",
    "print(f\"Percentage of unstable states: {unstable_percentage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total timesteps: 15192\n",
      "Created 633 bins with 24 steps\n",
      "Mean total normalized reward per episode from all bins: -1.770329\n"
     ]
    }
   ],
   "source": [
    "all_steps_np = np.concatenate(episodes_np, axis=0)\n",
    "total_steps_available = all_steps_np.shape[0]\n",
    "print(f\"Total timesteps: {total_steps_available}\")\n",
    "\n",
    "# Group the data in bins with 24 steps\n",
    "episode_length = 24\n",
    "num_bins = total_steps_available // episode_length\n",
    "#creates bins and fits the samples into 3D\n",
    "binned_steps = all_steps_np[:num_bins * episode_length].reshape(num_bins, episode_length, -1)\n",
    "print(f\"Created {num_bins} bins with {episode_length} steps\")\n",
    "# Randomly select 100 of these bins\n",
    "num_episodes_to_sample = 633\n",
    "#no replacement\n",
    "sampled_bin_indices = np.random.choice(num_bins, num_episodes_to_sample, replace=False)\n",
    "\n",
    "all_episode_rewards = []\n",
    "\n",
    "# Loop through the 100 randomly selected bins\n",
    "for bin_idx in sampled_bin_indices:\n",
    "    # Get one bin\n",
    "    episode_bin = binned_steps[bin_idx]\n",
    "    \n",
    "    normalized_rewards_for_episode = []\n",
    "    # Loop through each of the 24 steps in the bin\n",
    "    for step_data in episode_bin:\n",
    "        step_tensor = torch.tensor(step_data, dtype=torch.float32)\n",
    "        raw_reward = compute_reward_smooth(step_tensor.unsqueeze(0))\n",
    "        \n",
    "        # Apply normalization from rl_env\n",
    "        norm_reward = (raw_reward + 4) / 5\n",
    "        clipped_reward = np.clip(norm_reward, -1.0, 1.0)\n",
    "        normalized_rewards_for_episode.append(clipped_reward)\n",
    "    \n",
    "    # Find the total reward for this simulated episode\n",
    "    total_episode_reward = sum(normalized_rewards_for_episode)\n",
    "    all_episode_rewards.append(total_episode_reward)\n",
    "\n",
    "mean_total_episode_reward = np.mean(all_episode_rewards)\n",
    "\n",
    "print(f\"Mean total normalized reward per episode from all bins: {mean_total_episode_reward:.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
