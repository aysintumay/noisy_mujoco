{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from cost_func import compute_acp_cost, compute_map_physician_air, is_stable, compute_hr_physician_air, compute_pulsatility_physician_air, unstable_percentage, aggregate_air_physician, weaning_score_physician, super_metric\n",
    "from reward_func import compute_reward_smooth\n",
    "from model import WorldModel\n",
    "\n",
    "DATA_PATH = \"/abiomed/downsampled/10min_1hr_all_data.pkl\"\n",
    "#this is just for the visualization so it isn't messy\n",
    "MAX_STEPS_TO_PLOT = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in dataset: ['train', 'val', 'test', 'mean', 'std']\n",
      "Shape of combined set: 17865\n"
     ]
    }
   ],
   "source": [
    "with open(DATA_PATH, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "episodes = np.concatenate((data['train'],data['val'], data['test']), axis=0)\n",
    "print(f\"Keys in dataset: {list(data.keys())}\")\n",
    "print(f\"Shape of combined set: {len(episodes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to numpy if tensor\n",
    "if isinstance(episodes, torch.Tensor):    \n",
    "    episodes_np = episodes.numpy()  \n",
    "else:\n",
    "    episodes_np = np.array(episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ACP per timestep accounting for between episodes: 0.073368\n"
     ]
    }
   ],
   "source": [
    "actions_all = episodes_np[:, :, -1]\n",
    "\n",
    "\n",
    "#in order to account for the actions between episodes i am just treating the entire time series like an episode\n",
    "flattened_actions = actions_all.flatten()\n",
    "flattened_acp = compute_acp_cost(flattened_actions)\n",
    "\n",
    "#this is averaged across all actions since the acp function called sums acp per episode\n",
    "acp_per_timestep = flattened_acp/(len(flattened_actions)-1)\n",
    "print(f\"Mean ACP per timestep accounting for between episodes: {acp_per_timestep:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[95.51750488 97.64806684 99.48813273 ... 55.35730673 58.03034655\n",
      " 52.48650665]\n",
      "Overall AIR across all timesteps accounting for between episodes for MAP: 0.069607\n"
     ]
    }
   ],
   "source": [
    "flattened_actions = episodes_np[:, :, -1].flatten()\n",
    "#i am just using all of the steps instead of the split up episodes\n",
    "flattened_map = episodes_np[:, :, 0].flatten()\n",
    "overall_map_score = compute_map_physician_air(flattened_map, flattened_actions)\n",
    "\n",
    "print(flattened_map)\n",
    "if overall_map_score is not None:\n",
    "    print(f\"Overall AIR across all timesteps accounting for between episodes for MAP: {overall_map_score:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall AIR across all timesteps accounting for between episodes for HR: 0.071517\n"
     ]
    }
   ],
   "source": [
    "flattened_actions = episodes_np[:, :, -1].flatten()\n",
    "flattened_hr = episodes_np[:, :, 9].flatten()\n",
    "overall_hr_score = compute_hr_physician_air(flattened_hr, flattened_actions)\n",
    "if overall_hr_score is not None:\n",
    "    print(f\"Overall AIR across all timesteps accounting for between episodes for HR: {overall_hr_score:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall AIR across all timesteps accounting for between episodes for pulsatility: 0.074155\n"
     ]
    }
   ],
   "source": [
    "flattened_actions = episodes_np[:, :, -1].flatten()\n",
    "flattened_pulsatility = episodes_np[:, :, 7].flatten()\n",
    "overall_pulsatility_score = compute_pulsatility_physician_air(flattened_pulsatility, flattened_actions)\n",
    "if overall_pulsatility_score is not None:\n",
    "    print(f\"Overall AIR across all timesteps accounting for between episodes for pulsatility: {overall_pulsatility_score:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of unstable states: 17.822558074447244\n"
     ]
    }
   ],
   "source": [
    "states_all = episodes_np[:, :, :-1]\n",
    "flattened_states = states_all.reshape(-1, states_all.shape[-1])\n",
    "unstable_percentage = unstable_percentage(flattened_states)\n",
    "print(f\"Percentage of unstable states: {unstable_percentage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall weaning score -0.09487457555058966\n"
     ]
    }
   ],
   "source": [
    "overall_weaning_score = weaning_score_physician(flattened_states, flattened_actions)\n",
    "print(f\"Overall weaning score {overall_weaning_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregate AIR: 0.07374862702024164\n"
     ]
    }
   ],
   "source": [
    "total_air = aggregate_air_physician(flattened_states, flattened_actions)\n",
    "print(f\"Aggregate AIR: {total_air}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.55175049e+01 3.89947638e+03 2.37867920e+02 3.83526803e+01\n",
      " 1.39765734e+01 1.16824373e+02 8.12071603e+01 3.56172133e+01\n",
      " 5.34618863e+02 6.60000000e+01 7.08239484e-01 6.19862945e+01]\n",
      "Super Metric ACP: 6809.057172816964\n",
      "Super Metric ACP per timestep: 0.03176177318122094\n"
     ]
    }
   ],
   "source": [
    "print(flattened_states[0])\n",
    "super_acp = super_metric(flattened_states, flattened_actions)\n",
    "acp_timestep = super_acp/(len(flattened_actions)-1)\n",
    "print(f\"Super Metric ACP: {super_acp}\")\n",
    "print(f\"Super Metric ACP per timestep: {acp_timestep}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total timesteps: 214380\n",
      "Created 8932 bins with 24 steps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean total normalized reward per episode from all bins: -13.496964\n"
     ]
    }
   ],
   "source": [
    "#look at the next cell for reward\n",
    "all_steps_np = np.concatenate(episodes_np, axis=0)\n",
    "total_steps_available = all_steps_np.shape[0]\n",
    "print(f\"Total timesteps: {total_steps_available}\")\n",
    "\n",
    "# Group the data in bins with 24 steps\n",
    "episode_length = 24\n",
    "num_bins = total_steps_available // episode_length\n",
    "#creates bins and fits the samples into 3D\n",
    "binned_steps = all_steps_np[:num_bins * episode_length].reshape(num_bins, episode_length, -1)\n",
    "print(f\"Created {num_bins} bins with {episode_length} steps\")\n",
    "# Randomly select 100 of these bins\n",
    "num_episodes_to_sample = 633\n",
    "#no replacement\n",
    "sampled_bin_indices = np.random.choice(num_bins, num_episodes_to_sample, replace=False)\n",
    "\n",
    "all_episode_rewards = []\n",
    "\n",
    "# Loop through the 100 randomly selected bins\n",
    "for bin_idx in sampled_bin_indices:\n",
    "    # Get one bin\n",
    "    episode_bin = binned_steps[bin_idx]\n",
    "    \n",
    "    normalized_rewards_for_episode = []\n",
    "    # Loop through each of the 24 steps in the bin\n",
    "    for step_data in episode_bin:\n",
    "        step_tensor = torch.tensor(step_data, dtype=torch.float32)\n",
    "        raw_reward = compute_reward_smooth(step_tensor.unsqueeze(0))\n",
    "        \n",
    "        # Apply normalizatio\n",
    "        norm_reward = (raw_reward + 4) / 5\n",
    "        clipped_reward = np.clip(norm_reward, -1.0, 1.0)\n",
    "        normalized_rewards_for_episode.append(clipped_reward)\n",
    "    \n",
    "    # Find the total reward for this simulated episode\n",
    "    total_episode_reward = sum(normalized_rewards_for_episode)\n",
    "    all_episode_rewards.append(total_episode_reward)\n",
    "\n",
    "mean_total_episode_reward = np.mean(all_episode_rewards)\n",
    "\n",
    "print(f\"Mean total normalized reward per episode from all bins: {mean_total_episode_reward:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean normalized reward per 6hour episode for 100 episodes: 5.6654\n"
     ]
    }
   ],
   "source": [
    "EPISODE_LENGTH_STEPS = 36\n",
    "num_episodes = len(all_steps_np) // EPISODE_LENGTH_STEPS\n",
    "episodes_6hr = all_steps_np[:num_episodes * EPISODE_LENGTH_STEPS].reshape(\n",
    "    num_episodes, EPISODE_LENGTH_STEPS, -1\n",
    ")\n",
    "num_to_evaluate = 100\n",
    "if len(episodes_6hr) < num_to_evaluate:\n",
    "    num_to_evaluate = len(episodes_6hr)\n",
    "\n",
    "#just 100 eval episodes for reward\n",
    "episodes_to_evaluate = episodes_6hr[:num_to_evaluate]\n",
    "all_normalized_rewards = []\n",
    "\n",
    "for episode in episodes_to_evaluate:\n",
    "    total_normalized_reward = 0\n",
    "    for step_data in episode:\n",
    "        step_tensor = torch.tensor(step_data, dtype=torch.float32)\n",
    "        reward = compute_reward_smooth(step_tensor)\n",
    "        norm_reward = (reward + 8.93) / 4.41\n",
    "        clipped_reward = np.clip(norm_reward, -1.0, 1.0)\n",
    "        total_normalized_reward += clipped_reward.item()\n",
    "        \n",
    "    all_normalized_rewards.append(total_normalized_reward)\n",
    "mean_normalized_reward = np.mean(all_normalized_rewards)\n",
    "\n",
    "print(f\"\\nMean normalized reward per 6hour episode for 100 episodes: {mean_normalized_reward:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
