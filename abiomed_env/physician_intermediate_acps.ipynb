{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from cost_func import compute_acp_cost,aggregate_air_model,weaning_score_model, compute_map_physician_air, is_stable, compute_hr_physician_air, compute_pulsatility_physician_air, unstable_percentage, aggregate_air_physician, weaning_score_physician, super_metric\n",
    "from reward_func import compute_reward_smooth\n",
    "from model import WorldModel\n",
    "from rl_env import AbiomedRLEnvFactory\n",
    "from cost_func import overall_acp_cost\n",
    "\n",
    "DATA_PATH = \"/abiomed/downsampled/10min_1hr_all_data.pkl\"\n",
    "#this is just for the visualization so it isn't messy\n",
    "MAX_STEPS_TO_PLOT = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in dataset: ['train', 'val', 'test', 'mean', 'std']\n",
      "Shape of combined set: 17865\n"
     ]
    }
   ],
   "source": [
    "with open(DATA_PATH, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "episodes = np.concatenate((data['train'],data['val'], data['test']), axis=0)\n",
    "# episodes = data['train']\n",
    "print(f\"Keys in dataset: {list(data.keys())}\")\n",
    "print(f\"Shape of combined set: {len(episodes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to numpy if tensor\n",
    "if isinstance(episodes, torch.Tensor):    \n",
    "    episodes_np = episodes.numpy()  \n",
    "else:\n",
    "    episodes_np = np.array(episodes)\n",
    "actions_all = episodes_np[:, :, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ACP per timestep accounting for between episodes: 0.073368\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "#in order to account for the actions between episodes i am just treating the entire time series like an episode\n",
    "flattened_actions = actions_all.flatten()\n",
    "flattened_acp = compute_acp_cost(flattened_actions)\n",
    "\n",
    "#this is averaged across all actions since the acp function called sums acp per episode\n",
    "acp_per_timestep = flattened_acp/(len(flattened_actions)-1)\n",
    "print(f\"Mean ACP per timestep accounting for between episodes: {acp_per_timestep:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[95.51750488 97.64806684 99.48813273 ... 55.35730673 58.03034655\n",
      " 52.48650665]\n",
      "Overall AIR across all timesteps accounting for between episodes for MAP: 0.075566\n"
     ]
    }
   ],
   "source": [
    "flattened_actions = episodes_np[:, :, -1].flatten()\n",
    "#i am just using all of the steps instead of the split up episodes\n",
    "flattened_map = episodes_np[:, :, 0].flatten()\n",
    "overall_map_score = compute_map_physician_air(flattened_map, flattened_actions)\n",
    "\n",
    "print(flattened_map)\n",
    "if overall_map_score is not None:\n",
    "    print(f\"Overall AIR across all timesteps accounting for between episodes for MAP: {overall_map_score:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall AIR across all timesteps accounting for between episodes for HR: 0.043157\n"
     ]
    }
   ],
   "source": [
    "flattened_actions = episodes_np[:, :, -1].flatten()\n",
    "flattened_hr = episodes_np[:, :, 9].flatten()\n",
    "overall_hr_score = compute_hr_physician_air(flattened_hr, flattened_actions)\n",
    "if overall_hr_score is not None:\n",
    "    print(f\"Overall AIR across all timesteps accounting for between episodes for HR: {overall_hr_score:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall AIR across all timesteps accounting for between episodes for pulsatility: 0.061362\n"
     ]
    }
   ],
   "source": [
    "flattened_actions = episodes_np[:, :, -1].flatten()\n",
    "flattened_pulsatility = episodes_np[:, :, 7].flatten()\n",
    "overall_pulsatility_score = compute_pulsatility_physician_air(flattened_pulsatility, flattened_actions)\n",
    "if overall_pulsatility_score is not None:\n",
    "    print(f\"Overall AIR across all timesteps accounting for between episodes for pulsatility: {overall_pulsatility_score:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of unstable states: 17.822558074447244\n"
     ]
    }
   ],
   "source": [
    "states_all = episodes_np[:, :, :-1]\n",
    "flattened_states = states_all.reshape(-1, states_all.shape[-1])\n",
    "unstable_percentage = unstable_percentage(flattened_states)\n",
    "print(f\"Percentage of unstable states: {unstable_percentage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall weaning score -0.024611756001038224\n"
     ]
    }
   ],
   "source": [
    "overall_weaning_score = weaning_score_physician(flattened_states, flattened_actions)\n",
    "print(f\"Overall weaning score {overall_weaning_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregate AIR: 0.030380987382089917\n"
     ]
    }
   ],
   "source": [
    "total_air = aggregate_air_physician(flattened_states, flattened_actions)\n",
    "print(f\"Aggregate AIR: {total_air}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.55175049e+01 3.89947638e+03 2.37867920e+02 3.83526803e+01\n",
      " 1.39765734e+01 1.16824373e+02 8.12071603e+01 3.56172133e+01\n",
      " 5.34618863e+02 6.60000000e+01 7.08239484e-01 6.19862945e+01]\n",
      "Super Metric ACP: 6809.057172816964\n",
      "Super Metric ACP per timestep: 0.03176177318122094\n"
     ]
    }
   ],
   "source": [
    "print(flattened_states[0])\n",
    "super_acp = super_metric(flattened_states, flattened_actions)\n",
    "acp_timestep = super_acp/(len(flattened_actions)-1)\n",
    "print(f\"Super Metric ACP: {super_acp}\")\n",
    "print(f\"Super Metric ACP per timestep: {acp_timestep}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at the next cell for reward\n",
    "all_steps_np = np.concatenate(episodes_np, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17865, 12, 13)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episodes_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total timesteps: 214380\n",
      "Created 8932 bins with 24 steps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean total normalized reward per episode from all bins: -13.496964\n"
     ]
    }
   ],
   "source": [
    "\n",
    "total_steps_available = all_steps_np.shape[0]\n",
    "print(f\"Total timesteps: {total_steps_available}\")\n",
    "\n",
    "# Group the data in bins with 24 steps\n",
    "episode_length = 24\n",
    "num_bins = total_steps_available // episode_length\n",
    "#creates bins and fits the samples into 3D\n",
    "binned_steps = all_steps_np[:num_bins * episode_length].reshape(num_bins, episode_length, -1)\n",
    "print(f\"Created {num_bins} bins with {episode_length} steps\")\n",
    "# Randomly select 100 of these bins\n",
    "num_episodes_to_sample = 633\n",
    "#no replacement\n",
    "sampled_bin_indices = np.random.choice(num_bins, num_episodes_to_sample, replace=False)\n",
    "\n",
    "all_episode_rewards = []\n",
    "\n",
    "# Loop through the 100 randomly selected bins\n",
    "for bin_idx in sampled_bin_indices:\n",
    "    # Get one bin\n",
    "    episode_bin = binned_steps[bin_idx]\n",
    "    \n",
    "    normalized_rewards_for_episode = []\n",
    "    # Loop through each of the 24 steps in the bin\n",
    "    for step_data in episode_bin:\n",
    "        step_tensor = torch.tensor(step_data, dtype=torch.float32)\n",
    "        raw_reward = compute_reward_smooth(step_tensor.unsqueeze(0))\n",
    "        \n",
    "        # Apply normalizatio\n",
    "        norm_reward = (raw_reward + 4) / 5\n",
    "        clipped_reward = np.clip(norm_reward, -1.0, 1.0)\n",
    "        normalized_rewards_for_episode.append(clipped_reward)\n",
    "    \n",
    "    # Find the total reward for this simulated episode\n",
    "    total_episode_reward = sum(normalized_rewards_for_episode)\n",
    "    all_episode_rewards.append(total_episode_reward)\n",
    "\n",
    "mean_total_episode_reward = np.mean(all_episode_rewards)\n",
    "\n",
    "print(f\"Mean total normalized reward per episode from all bins: {mean_total_episode_reward:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPISODE_LENGTH_STEPS = 36\n",
    "# num_episodes = len(all_steps_np) // EPISODE_LENGTH_STEPS\n",
    "# episodes_6hr = all_steps_np[:num_episodes * EPISODE_LENGTH_STEPS].reshape(\n",
    "#     num_episodes, EPISODE_LENGTH_STEPS, -1\n",
    "# )\n",
    "\n",
    "episodes_np_res = episodes_np.reshape(-1,6,13)\n",
    "num_to_evaluate = episodes_np_res.shape[0] #episodes_np_res.shape[0]\n",
    "if len(episodes_np_res) < num_to_evaluate:\n",
    "    num_to_evaluate = len(episodes_np_res)\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes_to_evaluate = episodes_np_res[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12051, 6, 13)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episodes_to_evaluate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4467, 12, 13)\n",
      "(4467, 6, 13)\n",
      "(744, 6, 6, 13)\n",
      "(744, 6, 6, 13)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sliced_data = episodes_np[::4]\n",
    "print(sliced_data.shape)\n",
    "sliced_states  = sliced_data[:,:6, :]\n",
    "print(sliced_states.shape)\n",
    "sliced_next_states = sliced_data[:, 6:, :]\n",
    "\n",
    "#reshape into N/6, 6, 6, 13\n",
    "sliced_state_episodic = sliced_states[:-3].reshape(-1, 6, 6, 13)\n",
    "print(sliced_state_episodic.shape)\n",
    "sliced_next_state_episodic = sliced_next_states[:-3].reshape(-1, 6, 6, 13)\n",
    "print(sliced_next_state_episodic.shape)\n",
    "sliced_data_episodic = sliced_data[:-3].reshape(-1, 6, 12, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_episodes = sliced_state_episodic.shape[0]\n",
    "episode_len = sliced_state_episodic.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(744, 6)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_episodes,episode_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean normalized reward per episode: 0.0796\n",
      "Mean ACP per episode: 1.7634\n",
      "Mean Aggregate AIR per episode: 0.4079\n",
      "Mean Super Metric per episode: 0.0000\n",
      "Mean Weaning Score per episode: -0.0604\n"
     ]
    }
   ],
   "source": [
    "#just 100 eval episodes for reward\n",
    "\n",
    "all_normalized_rewards = []\n",
    "total_map_air_sum          = 0\n",
    "total_hr_air_sum           = 0\n",
    "total_pulsatility_air_sum  = 0\n",
    "total_aggregate_air_sum    = 0\n",
    "total_super_sum            = 0\n",
    "avg_acp = 0\n",
    "wean_score_sum = 0\n",
    "\n",
    "for episode_idx in range(eval_episodes):\n",
    "    episode_states = []\n",
    "    episode_actions = []\n",
    "    total_normalized_reward = 0\n",
    "    for state_idx in range(episode_len):\n",
    "        state = torch.tensor(sliced_data_episodic[episode_idx, state_idx, :6, :-1], dtype=torch.float32).reshape(6,12)\n",
    "        next_state = torch.tensor(sliced_data_episodic[episode_idx, state_idx, 6:, :-1], dtype=torch.float32).reshape(6,12)\n",
    "        action = np.array(np.bincount(sliced_data_episodic[episode_idx, state_idx, 6:, -1].astype(int)).argmax())\n",
    "\n",
    "        reward = compute_reward_smooth(next_state)\n",
    "        norm_reward = (reward + 8.93) / 4.41\n",
    "        total_normalized_reward += np.clip(norm_reward, -2.0, 2.0) \n",
    "\n",
    "        episode_states.append(state.numpy())\n",
    "        episode_actions.append(action)\n",
    "\n",
    "    all_normalized_rewards.append(total_normalized_reward)\n",
    "    avg_acp += compute_acp_cost(episode_actions)\n",
    "    # total_map_air_sum          += compute_map_physician_air(episode_states, episode_actions)\n",
    "    # total_hr_air_sum           += compute_hr_physician_air( episode_states,episode_actions)\n",
    "    # total_pulsatility_air_sum  += compute_pulsatility_physician_air( episode_states, episode_actions)\n",
    "    total_aggregate_air_sum    += aggregate_air_physician(episode_states, episode_actions)\n",
    "    wean_score_sum             += weaning_score_physician(episode_states, episode_actions)\n",
    "\n",
    "    # total_super_sum            += super_metric(episode_states, episode_actions)\n",
    "mean_normalized_reward = np.mean(all_normalized_rewards)\n",
    "print(f\"Mean normalized reward per episode: {mean_normalized_reward:.4f}\")\n",
    "avg_acp /= eval_episodes\n",
    "print(f\"Mean ACP per episode: {avg_acp:.4f}\")\n",
    "\n",
    "total_aggregate_air_sum /= eval_episodes\n",
    "print(f\"Mean Aggregate AIR per episode: {total_aggregate_air_sum:.4f}\")\n",
    "total_super_sum /= eval_episodes\n",
    "print(f\"Mean Super Metric per episode: {total_super_sum:.4f}\")\n",
    "total_weaning_score = wean_score_sum / eval_episodes\n",
    "print(f\"Mean Weaning Score per episode: {total_weaning_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-based Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = [0,1,2,3,4,5,6,7,8,9,10,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'config' from '/home/ubuntu/noisy_mujoco/abiomed_env/config.py'>\n",
      "time series transformer device cuda:1\n",
      "Using device: cuda:1\n",
      "Model loaded from /abiomed/downsampled/models/10min_1hr_all_data_model.pth\n",
      "loaded datasets with length \n",
      " train:  12051 \n",
      " val:  1938 \n",
      " test:  3876\n",
      "Data loaded from /abiomed/downsampled/10min_1hr_all_data.pkl\n",
      "Continuous action space. Min action: -2.2312597235306466, Max action: 1.8085589693336617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1162834/3766584408.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  next_state = eval_env.world_model.step(torch.tensor(state).unsqueeze(0), int(action1)).squeeze(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 744 episodes: Return 1.051\n",
      "ACP 3.1599\n",
      "Aggregate AIR/ep: 0.05316\n",
      "Unstable hours (%): 0.0\n",
      "Weaning score: -0.04030017921146955\n",
      "Super metric: 1.44086\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "eval_env = AbiomedRLEnvFactory.create_env(\n",
    "\t\t\t\t\t\t\t\t\tmodel_name=\"10min_1hr_all_data\",\n",
    "\t\t\t\t\t\t\t\t\tmodel_path=None,\n",
    "\t\t\t\t\t\t\t\t\tdata_path=None,\n",
    "\t\t\t\t\t\t\t\t\tmax_steps=6,\n",
    "\t\t\t\t\t\t\t\t\taction_space_type=\"continuous\",\n",
    "\t\t\t\t\t\t\t\t\treward_type=\"smooth\",\n",
    "\t\t\t\t\t\t\t\t\tnormalize_rewards=True,\n",
    "\t\t\t\t\t\t\t\t\tseed=42,\n",
    "\t\t\t\t\t\t\t\t\tdevice = f\"cuda:1\" if torch.cuda.is_available() else \"cpu\",\n",
    "\t\t\t\t\t\t\t\t\t)\n",
    "avg_reward = 0.0\n",
    "avg_acp = 0.0\n",
    "\n",
    "# ---- aggregated metrics over episodes ----\n",
    "total_map_air_sum = 0.0\n",
    "total_hr_air_sum = 0.0\n",
    "total_pulsatility_air_sum = 0.0\n",
    "total_aggregate_air_sum = 0.0\n",
    "total_unstable_percentage_sum = 0.0\n",
    "total_super_sum = 0.0\n",
    "wean_score_sum = 0.0\n",
    "\n",
    "\n",
    "for k in range(eval_episodes):\n",
    "    ep_states = []           # store normalized states per step (like in _evaluate)\n",
    "    actions = []\n",
    "    for t in range(episode_len):\n",
    "        action = sliced_next_states[k, t, :, -1]\n",
    "        action1 = np.array(np.bincount(action.astype(int)).argmax())\n",
    "        state = sliced_states[k, t, :, col].transpose()\n",
    "        #normalize the state\n",
    "        state = (torch.tensor(state) - eval_env.world_model.mean[col]) / eval_env.world_model.std[col]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_state = eval_env.world_model.step(torch.tensor(state).unsqueeze(0), int(action1)).squeeze(0)\n",
    "        \n",
    "        reward = eval_env._compute_reward(next_state)\n",
    "        avg_reward += reward.item()  # accumulate raw reward\n",
    "\n",
    "        ep_states.append(state.numpy())   # store the *current* obs like _evaluate\n",
    "        actions.append(int(action1))  # store the *current* action like _evaluate\n",
    "\n",
    "    \n",
    "    ep_states_np = np.asarray(ep_states, dtype=np.float32) #normalized\n",
    "    action_np = np.asarray(actions, dtype=np.int32) #unnormalized\n",
    "\n",
    "   \n",
    "    wm = getattr(eval_env, 'world_model', None)\n",
    "    if wm is None:\n",
    "        wm = eval_env.world_model  # fallback to global `env` if that's how you access it\n",
    "\n",
    "    # AIR metrics\n",
    "    # total_map_air_sum          += compute_map_physician_air(wm, ep_states_np, eval_env.episode_actions)\n",
    "    # total_hr_air_sum           += compute_hr_physician_air(wm, ep_states_np, eval_env.episode_actions)\n",
    "    # total_pulsatility_air_sum  += compute_pulsatility_physician_air(wm, ep_states_np, eval_env.episode_actions)\n",
    "    avg_acp += compute_acp_cost(action_np)\n",
    "    total_aggregate_air_sum    += aggregate_air_model(wm, ep_states_np, action_np)\n",
    "    total_super_sum            += super_metric(wm, ep_states_np, action_np)\n",
    "    wean_score_sum             += weaning_score_model(wm, ep_states_np, action_np)\n",
    "    # total_unstable_percentage_sum += unstable_percentage(wm, ep_states_np)   \n",
    "\n",
    "# ---- episode averages ----\n",
    "avg_reward /= eval_episodes\n",
    "acp_mean = avg_acp / eval_episodes\n",
    "\n",
    "map_air_mean          = total_map_air_sum / eval_episodes\n",
    "hr_air_mean           = total_hr_air_sum / eval_episodes\n",
    "puls_air_mean         = total_pulsatility_air_sum / eval_episodes\n",
    "aggregate_air_mean    = total_aggregate_air_sum / eval_episodes\n",
    "unstable_hours_mean   = total_unstable_percentage_sum / eval_episodes\n",
    "weaning_score_mean    = wean_score_sum / eval_episodes\n",
    "super_mean            = total_super_sum / eval_episodes\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(f\"Evaluation over {eval_episodes} episodes: \"\n",
    "        f\"Return {avg_reward:.3f}\")\n",
    "print(f\"ACP {acp_mean:.4f}\")\n",
    "# print(f\"MAP AIR/ep: {map_air_mean:.5f} | HR AIR/ep: {hr_air_mean:.5f} \"\n",
    "#         f\"| Pulsatility AIR/ep: {puls_air_mean:.5f}\")\n",
    "print(f\"Aggregate AIR/ep: {aggregate_air_mean:.5f}\")\n",
    "print(f\"Unstable hours (%): {unstable_hours_mean}\")\n",
    "print(f\"Weaning score: {weaning_score_mean}\")\n",
    "print(f\"Super metric: {super_mean:.5f}\")\n",
    "print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mopo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
