{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cvar_planning.ipynb  model_improvement.ipynb  rl.ipynb\n",
      "downsample.ipynb     model_vis.ipynb          uq_10min1hr_50samples.npy\n",
      "explore.ipynb        planning.ipynb           workspace.ipynb\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Down sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10s per sample -> 5 minutes per sample\n",
    "\n",
    "downsampled = []\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    with open('/abiomed-tmp/intermediate/at_data_amicgs_{}.pkl'.format(i), 'rb') as f:\n",
    "        data_amicgs = pickle.load(f)\n",
    "    \n",
    "    for j in range(len(data_amicgs)):\n",
    "        struct = data_amicgs[j]\n",
    "        id = struct['patient_id']\n",
    "        data = struct['data']\n",
    "        # Downsample by taking the average of every 30 samples\n",
    "        if len(data) >= 30:\n",
    "            # Reshape to group every 30 samples and take mean\n",
    "            num_complete_groups = len(data) // 30\n",
    "            reshaped_data = data[:num_complete_groups * 30].reshape(num_complete_groups, 30, -1)\n",
    "            downsampled_data = reshaped_data.mean(axis=1)\n",
    "            downsampled.append({\n",
    "                'patient_id': id,\n",
    "                'data': downsampled_data\n",
    "            })\n",
    "    \n",
    "# # Save downsampled data as pickle\n",
    "# with open('/abiomed-tmp/intermediate/at_5min_data.pkl', 'wb') as f:\n",
    "#     pickle.dump(downsampled, f)\n",
    "\n",
    "# print(f\"Saved {len(downsampled)} downsampled patient records\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1325 downsampled patient records\n"
     ]
    }
   ],
   "source": [
    "# 10s -> 10 minutes per sample\n",
    "\n",
    "downsampled = []\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    with open('/abiomed-tmp/intermediate/at_data_amicgs_{}.pkl'.format(i), 'rb') as f:\n",
    "        data_amicgs = pickle.load(f)\n",
    "    \n",
    "    for j in range(len(data_amicgs)):\n",
    "        struct = data_amicgs[j]\n",
    "        id = struct['patient_id']\n",
    "        data = struct['data']\n",
    "        # Downsample by taking the average of every 60 samples\n",
    "        if len(data) >= 60:\n",
    "            # Reshape to group every 30 samples and take mean\n",
    "            num_complete_groups = len(data) // 60\n",
    "            reshaped_data = data[:num_complete_groups * 60].reshape(num_complete_groups, 60, -1)\n",
    "            downsampled_data = reshaped_data.mean(axis=1)\n",
    "            downsampled.append({\n",
    "                'patient_id': id,\n",
    "                'data': downsampled_data\n",
    "            })\n",
    "    \n",
    "# # Save downsampled data as pickle\n",
    "# with open('/abiomed-tmp/intermediate/at_10min_data.pkl', 'wb') as f:\n",
    "#     pickle.dump(downsampled, f)\n",
    "\n",
    "# print(f\"Saved {len(downsampled)} downsampled patient records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled = []\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    with open('/abiomed/intermediate/low_score_amicgs/at_data_amicgs_{}.pkl'.format(i), 'rb') as f:\n",
    "        data_amicgs = pickle.load(f)\n",
    "    \n",
    "    for j in range(len(data_amicgs)):\n",
    "        struct = data_amicgs[j]\n",
    "        id = struct['patient_id']\n",
    "        data = struct['data']\n",
    "        # Downsample by taking the average of every 60 samples\n",
    "        if len(data) >= 60:\n",
    "            # Reshape to group every 30 samples and take mean\n",
    "            num_complete_groups = len(data) // 60\n",
    "            reshaped_data = data[:num_complete_groups * 60].reshape(num_complete_groups, 60, -1)\n",
    "            downsampled_data = reshaped_data.mean(axis=1)\n",
    "            downsampled.append({\n",
    "                'patient_id': id,\n",
    "                'data': downsampled_data\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 436 downsampled patient records\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "# Save downsampled data as pickle\n",
    "with open('/abiomed/intermediate/low_score_amicgs/at_10min_data.pkl', 'wb') as f:\n",
    "    pickle.dump(downsampled, f)\n",
    "\n",
    "print(f\"Saved {len(downsampled)} downsampled patient records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_windows(data, window_size, stride):\n",
    "    Xs = []\n",
    "\n",
    "    if len(data) > window_size:  \n",
    "        # Only create windows if we have enough data points\n",
    "        i = 0\n",
    "        while i < len(data) - window_size:\n",
    "            Xs.append(data[i:i+window_size])\n",
    "            i += stride\n",
    "        Xs = torch.stack(Xs)\n",
    "    return Xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal data 10min 1325 torch.Size([4, 13])\n",
      "low score 10min 436 torch.Size([50, 13])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with open('/abiomed/intermediate/at_10min_data.pkl', 'rb') as f:\n",
    "    downsampled = pickle.load(f)\n",
    "\n",
    "print(\"Normal data 10min\", len(downsampled), downsampled[0]['data'].shape)\n",
    "#80% for training, 20% for testing\n",
    "train_downsampled = downsampled[:int(len(downsampled) * 0.8)]\n",
    "test_downsampled = downsampled[int(len(downsampled) * 0.8):]\n",
    "\n",
    "\n",
    "\n",
    "with open('/abiomed/intermediate/low_score_amicgs/at_10min_data.pkl', 'rb') as f:\n",
    "    downsampled = pickle.load(f)\n",
    "\n",
    "print(\"low score 10min\", len(downsampled), downsampled[0]['data'].shape)\n",
    "#80% for training, 20% for testing\n",
    "train_downsampled = downsampled[:int(len(downsampled) * 0.8)]\n",
    "test_downsampled = downsampled[int(len(downsampled) * 0.8):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal data 10min 1325 samples torch.Size([4, 13])\n",
      "low score 10min 436 samples torch.Size([50, 13])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with open('/abiomed/intermediate/at_10min_data.pkl', 'rb') as f:\n",
    "    downsampled = pickle.load(f)\n",
    "\n",
    "print(\"Normal data 10min\", len(downsampled) , \"samples\", downsampled[0]['data'].shape)\n",
    "#75% for training, 25% for testing + validation\n",
    "train_downsampled_normal = downsampled[:int(len(downsampled) * 0.8)]\n",
    "test_downsampled_normal = downsampled[int(len(downsampled) * 0.8):]\n",
    "\n",
    "\n",
    "with open('/abiomed/intermediate/low_score_amicgs/at_10min_data.pkl', 'rb') as f:\n",
    "    downsampled = pickle.load(f)\n",
    "\n",
    "print(\"low score 10min\", len(downsampled), \"samples\", downsampled[0]['data'].shape)\n",
    "#75% for training, 25% for testing + validation\n",
    "train_downsampled_low_score = downsampled[:int(len(downsampled) * 0.8)]\n",
    "test_downsampled_low_score = downsampled[int(len(downsampled) * 0.8):]\n",
    "\n",
    "\n",
    "train_all = train_downsampled_normal + train_downsampled_low_score\n",
    "test_all = test_downsampled_normal + test_downsampled_low_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_1hr.shape torch.Size([12051, 12, 13])\n",
      "test_1hr.shape torch.Size([3876, 12, 13])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_1hr = []\n",
    "test_1hr = []\n",
    "\n",
    "window_size = 12\n",
    "stride = 3\n",
    "\n",
    "for struct in train_all:  \n",
    "    data = struct['data']\n",
    "    xs = create_windows(data,window_size,stride)\n",
    "    if len(xs) > 4: # more than 4 hours in length\n",
    "        train_1hr.append(xs)\n",
    "\n",
    "for struct in test_all:\n",
    "    data = struct['data']\n",
    "    xs = create_windows(data,window_size,stride)\n",
    "    if len(xs) > 4: # more than 4 hours in length\n",
    "        test_1hr.append(xs)\n",
    "\n",
    "train_1hr = torch.cat(train_1hr)\n",
    "test_1hr = torch.cat(test_1hr)\n",
    "\n",
    "print(\"train_1hr.shape\",train_1hr.shape)\n",
    "print(\"test_1hr.shape\",test_1hr.shape)\n",
    "\n",
    "mean = torch.mean(train_1hr.reshape(-1, 13), axis=0)\n",
    "std = torch.std(train_1hr.reshape(-1, 13), axis=0)\n",
    "\n",
    "# randomly sample half of the test data for validation\n",
    "val_data = test_1hr[torch.randperm(test_1hr.shape[0])[:len(test_1hr)//2]]\n",
    "\n",
    "#dict_keys(['train', 'val', 'test', 'mean', 'std'])\n",
    "\n",
    "with open('/abiomed/downsampled/10min_1hr_all_data.pkl', 'wb') as f:\n",
    "    pickle.dump({'train': train_1hr, 'val': val_data, 'test': test_1hr, 'mean': mean, 'std': std}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13])\n"
     ]
    }
   ],
   "source": [
    "print(mean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_xs_1hr.shape torch.Size([4881, 24, 13])\n",
      "test_xs_1hr.shape torch.Size([1440, 24, 13])\n"
     ]
    }
   ],
   "source": [
    "# # input is 1hr, output is 1hr\n",
    "\n",
    "# train_xs = []\n",
    "# test_xs = []\n",
    "\n",
    "# window_size = 24\n",
    "# stride = 12\n",
    "\n",
    "# for struct in train_downsampled:  \n",
    "#     data = struct['data']\n",
    "#     xs = create_windows(data,window_size,stride)\n",
    "#     if len(xs) > 4: # more than 4 hours in length\n",
    "#         train_xs.append(xs)\n",
    "\n",
    "# for struct in test_downsampled:\n",
    "#     data = struct['data']\n",
    "#     xs = create_windows(data,window_size,stride)\n",
    "#     if len(xs) > 4: # more than 4 hours in length\n",
    "#         test_xs.append(xs)\n",
    "\n",
    "# train_xs = torch.cat(train_xs)\n",
    "# test_xs = torch.cat(test_xs)\n",
    "\n",
    "# print(\"train_xs.shape\",train_xs.shape)\n",
    "# print(\"test_xs.shape\",test_xs.shape)\n",
    "# with open('/abiomed-tmp/downsampled/5min_1hr_window.pkl', 'wb') as f:\n",
    "#     pickle.dump({'train': train_xs_1hr, 'test': test_xs_1hr}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open('/abiomed-tmp/intermediate/at_10min_data.pkl', 'rb') as f:\n",
    "    downsampled = pickle.load(f)\n",
    "\n",
    "print(len(downsampled))\n",
    "#80% for training, 20% for testing\n",
    "train_downsampled = downsampled[:int(len(downsampled) * 0.8)]\n",
    "test_downsampled = downsampled[int(len(downsampled) * 0.8):]\n",
    "\n",
    "train_xs_1hr = []\n",
    "test_xs_1hr = []\n",
    "\n",
    "window_size = 60\n",
    "stride = 30\n",
    "\n",
    "for struct in train_downsampled:  \n",
    "    data = struct['data']\n",
    "    xs = create_windows(data,window_size,stride)\n",
    "    if len(xs) > 4: # more than 4 hours in length\n",
    "        train_xs_1hr.append(xs)\n",
    "\n",
    "for struct in test_downsampled:\n",
    "    data = struct['data']\n",
    "    xs = create_windows(data,window_size,stride)\n",
    "    if len(xs) > 4: # more than 4 hours in length\n",
    "        test_xs_1hr.append(xs)\n",
    "\n",
    "train_xs_1hr = torch.cat(train_xs_1hr)\n",
    "test_xs_1hr = torch.cat(test_xs_1hr)\n",
    "\n",
    "print(\"train_xs_1hr.shape\",train_xs_1hr.shape)\n",
    "print(\"test_xs_1hr.shape\",test_xs_1hr.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1325 patients in total\n",
    "# 896 greater than 1 hour\n",
    "# 640 greater than 2 hour\n",
    "# 357 greater than 5 hour\n",
    "# 87 greater than 24 hour\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
